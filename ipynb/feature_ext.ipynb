{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec averaged repr. of a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training word2vec on rock corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim,os\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "\n",
    "train_path='rank/train/'\n",
    "sentences = MySentences(train_path) \n",
    "model = gensim.models.Word2Vec(sentences,size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfdf11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v1=make_feature_vec('I love you',model,100)\n",
    "v2=make_feature_vec('I love you baby',model,100)\n",
    "v3=make_feature_vec('moon is shining',model,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0280046085098\n",
      "0.99573738118\n",
      "0.988993420945\n"
     ]
    }
   ],
   "source": [
    "print cosine(v1,v2)\n",
    "print cosine(v1,v3)\n",
    "print cosine(v2,v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_word(word): \n",
    "    return re.sub(u'[^A-Za-z]','',word)\n",
    "\n",
    "def make_feature_vec(words, model, num_features):\n",
    "    # average all of the word vectors in a sentence\n",
    "    #words=nlp(words)\n",
    "    #words=[token.lemma_ for token in words]\n",
    "    words=words.split(' ')\n",
    "    #print (words)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    for word in words:\n",
    "        word = clean_word(word)\n",
    "        if word not in model:\n",
    "            #print(word)\n",
    "            nlp=spacy.load('en')\n",
    "            word = nlp(word)\n",
    "            try:\n",
    "                word = [i.lemma_ for i in word][0]\n",
    "            except IndexError:\n",
    "                continue\n",
    "\n",
    "        if word not in model:\n",
    "            continue\n",
    "\n",
    "        featureVec = np.add(featureVec,model[word])\n",
    "        nwords = nwords + 1\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save, load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname='rock_train.w2v'\n",
    "#model.save(fname)\n",
    "model = Word2Vec.load(fname)  # you can continue training with the loaded model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gensim modules: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import smart_open\n",
    "\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "\n",
    "def remove_empty_lines(fn):\n",
    "    f=open(fn,'r').read().split('\\n')\n",
    "    output=fn.replace('.txt','_noemp.txt')\n",
    "    open(output,'w').close\n",
    "    g=open(output,'a')\n",
    "    for line in f:\n",
    "        if line!='':\n",
    "            g.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean file to remove empty lines if necessary\n",
    "train_file='rank/train/lyrics_train_data_clean.txt'\n",
    "test_file='rank/test/lyrics_test_data_clean.txt'\n",
    "remove_empty_lines(train_file)\n",
    "remove_empty_lines(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preprocess corpus into gensim format\n",
    "train_file='rank/train/lyrics_train_data_clean_noemp.txt'\n",
    "test_file='rank/test/lyrics_test_data_clean_noemp.txt'\n",
    "train_corpus = list(read_corpus(train_file))\n",
    "test_corpus = list(read_corpus(test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=[u'may', u'seem', u'unusual', u'but', u'there', u'is', u'really', u'nothing', u'wrong', u'with', u'me'], tags=[0]),\n",
       " TaggedDocument(words=[u'tell', u'you', u'my', u'secrets', u'but', u'you', u'think', u'that', u'boring'], tags=[1])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(size=50, min_count=2)\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print epoch\n",
    "    model.train(train_corpus, total_examples=model.corpus_count)\n",
    "    model.alpha -= 0.002  \n",
    "    model.min_alpha = model.alpha  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alternative training\n",
    "model = gensim.models.doc2vec.Doc2Vec(size=50, min_count=2,iter=15)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname='rock_train.d2v'\n",
    "#model.save(fname)\n",
    "model = Doc2Vec.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.296794090269\n",
      "0.952921933758\n",
      "0.901668460463\n"
     ]
    }
   ],
   "source": [
    "x='Oh never let you go'\n",
    "y=\"I'll never let you go\"\n",
    "z=\"It's my turn, searching for another break\"\n",
    "\n",
    "\n",
    "\n",
    "a=model.infer_vector(x.split(' '))\n",
    "b=model.infer_vector(y.split(' '))\n",
    "c=model.infer_vector(z.split(' '))\n",
    "print cosine(a,b)\n",
    "print cosine(a,c)\n",
    "print cosine(b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77862892319\n",
      "1.00376286212\n",
      "0.357445432395\n",
      "0.542003353344\n"
     ]
    }
   ],
   "source": [
    "test_pas=\"\"\"I wanna love you like it ain't no secret\n",
    "Like I'm not ashamed to show\n",
    "Nor would I ever, oh never never\n",
    "Oh never let you go\n",
    "I'll never let you go\"\"\"\n",
    "\n",
    "t=test_pas.split('\\n')\n",
    "e=model.infer_vector(t[0].split(' '))\n",
    "a=model.infer_vector(t[1].split(' '))\n",
    "b=model.infer_vector(t[2].split(' '))\n",
    "c=model.infer_vector(t[3].split(' '))\n",
    "d=model.infer_vector(t[4].split(' '))\n",
    "print cosine(e,a)\n",
    "print cosine(a,b)\n",
    "print cosine(b,c)\n",
    "print cosine(c,d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'wanna', 'love', 'you', 'like', 'it', \"ain't\", 'no', 'secret'], ['Like', \"I'm\", 'not', 'ashamed', 'to', 'show'], ['Nor', 'would', 'I', 'ever,', 'oh', 'never', 'never'], ['Oh', 'never', 'let', 'you', 'go'], [\"I'll\", 'never', 'let', 'you', 'go']]\n"
     ]
    }
   ],
   "source": [
    "test_pas=\"\"\"I wanna love you like it ain't no secret\n",
    "Like I'm not ashamed to show\n",
    "Nor would I ever, oh never never\n",
    "Oh never let you go\n",
    "I'll never let you go\"\"\"\n",
    "\n",
    "t=test_pas.split('\\n')\n",
    "vec=[]\n",
    "for line in t:\n",
    "    l=line.split(' ')\n",
    "    vec.append(l)\n",
    "print vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b=model.infer_vector(vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('model/GoogleNews-vectors-negative300.bin', binary=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    # average all of the word vectors in a sentence\n",
    "    words=words.split(' ')\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    for word in words:\n",
    "        nwords = nwords + 1.\n",
    "        featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v1=make_feature_vec('I love you',model,300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx = np.nan_to_num(v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[[-1.9321557680765789, 0.77957312279103508, 0.50855190776400561, 0.34873870429582832, 0, 0, 40, 28, 1], [-3.41674314226423, 0.48945975253308649, 0.59788698690639319, 0.35616330825118925, 0, 1, 28, 32, 1], [-1.721189308166504, 0.52949953423100071, 0.26509378411068274, 0.19793695619404383, 0, 0, 32, 19, 1], [-1.1092583656311035, 0.26562739714544592, 0.10523526352690327, 0.070908816034415123, 1, 0, 19, 21, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.9321557680765789, 0.7795731227910351, 0.5085519077640056, 0.3487387042958283, 0, 0, 40, 28, 1]\n",
      "[-3.41674314226423, 0.4894597525330865, 0.5978869869063932, 0.35616330825118925, 0, 1, 28, 32, 1]\n",
      "[-1.721189308166504, 0.5294995342310007, 0.26509378411068274, 0.19793695619404383, 0, 0, 32, 19, 1]\n",
      "[-1.1092583656311035, 0.2656273971454459, 0.10523526352690327, 0.07090881603441512, 1, 0, 19, 21, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in a: print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=[[-2.9132102966308593, 0.84677078160842412, 0.77228926608025872, 0.73800714765249742, 0, 0, 25, 20, 1], [-2.3162093692355685, 0.85949340103096816, 0.80950776360602306, 0.69511529949504136, 0, 0, 20, 50, 1], [-2.026075839996338, 0.79122691193538486, 0.69034124896942539, 0.576113041466773, 0, 0, 50, 18, 1], [-1.7705413273402624, 1.1136524156219227, 0.57432313199626739, 0.44441332938962597, 0, 0, 18, 65, 1], [-3.4620275497436523, 0.88504072570899339, 0.86365218545666289, 0.64107502369887337, 0, 0, 65, 21, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.9132102966308593, 0.8467707816084241, 0.7722892660802587, 0.7380071476524974, 0, 0, 25, 20, 1]\n",
      "[-2.3162093692355685, 0.8594934010309682, 0.8095077636060231, 0.6951152994950414, 0, 0, 20, 50, 1]\n",
      "[-2.026075839996338, 0.7912269119353849, 0.6903412489694254, 0.576113041466773, 0, 0, 50, 18, 1]\n",
      "[-1.7705413273402624, 1.1136524156219227, 0.5743231319962674, 0.44441332938962597, 0, 0, 18, 65, 1]\n",
      "[-3.4620275497436523, 0.8850407257089934, 0.8636521854566629, 0.6410750236988734, 0, 0, 65, 21, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in b: print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# right now it seems like LM is behaving not the best, but w2v, d2v, and googlew2v behave all kind of reasonable as a sanity check. we can proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background:blue;color:white;padding:2pt;\"> if you have time </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detecting gibberish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do better on detecting gibberish sentence here is an idea:\n",
    "\n",
    "1. build a data set contains 10000 randomly sampled sentences from lyrics;(real)\n",
    "\n",
    "2. generate 20,000 sents from the LM, rank them by loglik, then take the bottom half 10,000 to add to the data set;\n",
    "\n",
    "3. train this data set on d2v;\n",
    "\n",
    "4. train a doc classifier that classifies lyrics frmo gibberish lyrics;\n",
    "\n",
    "5. for ranking, for each candidate line generate a prediction of whether this is gibberish or not and add it to the feature set. when training the next line classifier, you can also compute this feature of course (for pos examples they should always be 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM\n",
    "\n",
    "now the smoothing (KN) will cause \"I am zangsir\" to have much lower likelihood than \"I am John\". Converting all proper names to a tag would help. But again, this would not happen within the system. And that'll have to be preprocessed before the training of LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# other text discourse coherence features:\n",
    " - vocabulary introduction: count the new vocab in consecutive blockes of texts\n",
    " - lexical chains: looks at a sliding window of text and see the lexical overlaps \n",
    "\n",
    "https://corpling.uis.georgetown.edu/compdisc/webtile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Rhyming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generic rhyming function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import pronouncing\n",
    "def is_rhyme(word1,word2):\n",
    "    return word1 in pronouncing.rhymes(word2)\n",
    "print is_rhyme('hood?','good')\n",
    "print is_rhyme('hood','hooded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_rhyme(word1,word2):\n",
    "    word1=re.sub(u'[^A-Za-z]','',word1)\n",
    "    word2=re.sub(u'[^A-Za-z]','',word2)\n",
    "    return int((word1 in pronouncing.rhymes(word2)) or word1==word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print is_rhyme('go','show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sent internal rhyming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a list of words, check if the last one rhymes with any one of the rest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "model = kenlm.LanguageModel('train.lm')\n",
    "sent='in the beginning was the word'\n",
    "loglik=model.score(sent)/len(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build training data\n",
    "\n",
    "- for a line, extract features based on this line and previous line, label=1 \n",
    "- for a line, extract features based on this line and a random line, label=0\n",
    "- generate a large number of sents, for a low loglik generated line (bottom ranked K in the normalized-loglik list), extract features for this line and a random line from the corpus, label=0\n",
    "- do we need to generate a lot of sentences for the negative examples?\n",
    "- we could also impose some hard coded constraints on the loglik threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'this', u'be', u'spacy', u'lemmatize', u'testing', u'.', u'programming', u'book', u'be', u'more', u'better', u'than', u'others']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from spacy.en import English\n",
    "\n",
    "#data_dir = os.environ.get('SPACY_DATA', LOCAL_DATA_DIR)\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "doc3 = nlp(u\"this is spacy lemmatize testing. programming books are more better than others\")\n",
    "\n",
    "print [token.lemma_ for token in doc3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'like', u'i', u'be', u'a', u'broken', u'relate', u'no', u'he', u'save', u'the', u'jam']\n"
     ]
    }
   ],
   "source": [
    "doc3=nlp(u\"Like I'm a broken relates No he saves the jams\")\n",
    "print [token.lemma_ for token in doc3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words=u\"Like I'm a broken relates No he saves the jams\"\n",
    "words=nlp(words)\n",
    "words=[token.lemma_ for token in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'like',\n",
       " u'i',\n",
       " u'be',\n",
       " u'a',\n",
       " u'broken',\n",
       " u'relate',\n",
       " u'no',\n",
       " u'he',\n",
       " u'save',\n",
       " u'the',\n",
       " u'jam']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello world"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp(unicode(\"Hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n",
      "i\n",
      "be\n",
      "a\n",
      "break\n",
      "relate\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u\"Like I'm a broken relates\")\n",
    "\n",
    "for token in doc3:\n",
    "    print token.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relates\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "doc3=\"relates\"\n",
    "for token in doc3.split(' '):\n",
    "    print lmtzr.lemmatize(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
