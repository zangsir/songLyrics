{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building train data\n",
    "\n",
    "-for a line, extract features based on this line and previous line, label=1\n",
    "\n",
    "-for a line, extract features based on this line and a random line, label=0\n",
    "\n",
    "-generate a large number of sents, for a low loglik generated line (bottom ranked K in the normalized-loglik list), \n",
    "\n",
    "-extract features for this line and a random line from the corpus, label=0\n",
    "\n",
    "-do we need to generate a lot of sentences for the negative examples?\n",
    "\n",
    "-we could also impose some hard coded constraints on the loglik threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11893\n"
     ]
    }
   ],
   "source": [
    "f=open('rank/test/lyrics_test_data_clean.txt','r').read().split('\\n\\n')\n",
    "print len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "But don't leave your keys for me, open the door\n",
      "Nobody's getting hurt here if nobody cares\n",
      "Nobody's getting lost if nobody's going anywhere\n",
      "---------\n",
      "And darling I know that you can't be sure of anything anymore\n",
      "---------\n",
      "I wanna love you like it ain't no secret\n",
      "Like I'm not ashamed to show\n",
      "Nor would I ever, oh never never\n",
      "Oh never let you go\n",
      "I'll never let you go\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "for i in f[:3]:\n",
    "    print i\n",
    "    print '---------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named kenlm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4a9ca96fe920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkenlm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named kenlm"
     ]
    }
   ],
   "source": [
    "import kenlm\n",
    "import gensim,os\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.models import Doc2Vec\n",
    "import pronouncing\n",
    "\n",
    "\n",
    "\n",
    "def is_rhyme(word1,word2):\n",
    "    word1=re.sub(u'[^A-Za-z]','',word1)\n",
    "    word2=re.sub(u'[^A-Za-z]','',word2)\n",
    "    return int(word1 in pronouncing.rhymes(word2))\n",
    "    \n",
    "    \n",
    "def is_rhyme_current(sent):\n",
    "    sent=sent.split(' ')\n",
    "    last_word=sent[-1]\n",
    "    for w in sent[:-1]:\n",
    "        if is_rhyme(w,last_word):\n",
    "            return 1\n",
    "    return 0\n",
    "    \n",
    "    \n",
    "def get_loglik_norm(sent,LM):\n",
    "    model = kenlm.LanguageModel(LM)\n",
    "    loglik_norm=model.score(sent)/len(sent.split(' '))\n",
    "    return loglik_norm\n",
    "\n",
    "def get_d2v_dist(sent1,sent2,model):\n",
    "    model = Doc2Vec.load(model)\n",
    "    a=model.infer_vector(sent1.split(' '))\n",
    "    b=model.infer_vector(sent2.split(' '))\n",
    "    return cosine(a,b)\n",
    "\n",
    "\n",
    "\n",
    "def make_feature_vec(words, model, num_features):\n",
    "    # average all of the word vectors in a sentence\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def get_w2v_dist(sent1,sent2,model):\n",
    "    v1=make_feature_vec(sent1,model,100)\n",
    "    v2=make_feature_vec(sent2,model,100)\n",
    "    return cosine(v1,v2)\n",
    "\n",
    "\n",
    "def extract_features_positive(passage,LM,w2v_model,d2v_model):\n",
    "    \"\"\"extract feature from one passage\"\"\"\n",
    "    # a passage is a consecutive set of lines without a blank line in between. we extract features with these pairs \n",
    "    # of lines as prev and next lines. they're a more coherent unit. The passages is obtained by methods above, \n",
    "    # namely, splitting the training file by '\\n\\n'\n",
    "    line_list=passage.split('\\n')\n",
    "    line_list=[i for i in line_list if i!='']\n",
    "    features=['loglik_norm','d2v_dist','w2v_dist','rhyme_prev','rhyme_current','len_prev','len_cur','label']\n",
    "    pos_feature_vec=[]\n",
    "    for i in range(1,len(line_list)):\n",
    "        #extract features from the current and prev line\n",
    "        prev=line_list[i-1]\n",
    "        current=line_list[i]\n",
    "        print('prev,current:')\n",
    "        print(prev,current)\n",
    "        #features\n",
    "        loglik_norm=get_loglik_norm(current,LM)#LM='train3.lm'\n",
    "        d2v_dist=get_d2v_dist(prev,current,d2v_model)\n",
    "        w2v_dist=get_w2v_dist(prev,current,w2v_model)\n",
    "        rhyme_prev=is_rhyme(prev.split(' ')[-1],current.split(' ')[-1])\n",
    "        rhyme_current=is_rhyme_current(current)\n",
    "        num_words_cur=len(current)\n",
    "        num_words_prev=len(prev)\n",
    "        label=1\n",
    "        pos_feature_vec.append([loglik_norm,d2v_dist,w2v_dist,rhyme_prev,rhyme_current,num_word_prev,num_words_cur,label])\n",
    "    return pos_feature_vec\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"But don't leave your keys for me, open the door\",\n",
       " \"Nobody's getting hurt here if nobody cares\",\n",
       " \"Nobody's getting lost if nobody's going anywhere\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2v_model='rock_train.d2v'\n",
    "w2v_model='rock_train.w2v'\n",
    "LM='train3.lm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pas=\"\"\"I wanna love you like it ain't no secret\n",
    "Like I'm not ashamed to show\n",
    "Nor would I ever, oh never never\n",
    "Oh never let you go\n",
    "I'll never let you go\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[[-1.9321557680765789, 0.78070413044238873, 0.022806050290745117, 0, 0, 40, 28, 1], [-3.41674314226423, 0.43240190787867261, 0.23933394083644333, 0, 1, 28, 32, 1], [-1.721189308166504, 0.61283882132826939, 0.078219546226546699, 0, 0, 32, 19, 1], [-1.1092583656311035, 0.5752945981640073, 0.16501156574460696, 1, 0, 19, 21, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#after fixing w2v\n",
    "a=[[-1.9321557680765789, 0.84771155311134316, 0.50855190776400561, 0, 0, 0, 40, 28, 1], [-3.41674314226423, 0.49395473391852707, 0.59788698690639319, 0, 0, 1, 28, 32, 1], [-1.721189308166504, 0.5148408640544081, 0.26509378411068274, 0, 0, 0, 32, 19, 1], [-1.1092583656311035, 0.25549408668095219, 0.10523526352690327, 0, 1, 0, 19, 21, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.9321557680765789, 0.8477115531113432, 0.5085519077640056, 0, 0, 0, 40, 28, 1]\n",
      "[-3.41674314226423, 0.49395473391852707, 0.5978869869063932, 0, 0, 1, 28, 32, 1]\n",
      "[-1.721189308166504, 0.5148408640544081, 0.26509378411068274, 0, 0, 0, 32, 19, 1]\n",
      "[-1.1092583656311035, 0.2554940866809522, 0.10523526352690327, 0, 1, 0, 19, 21, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in a:print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=[[-2.9132102966308593, 0.84254782164524356, 0.18192692156665857, 0, 0, 25, 20, 1], [-2.3162093692355685, 0.92273771747298061, 0.13780720014199244, 0, 0, 20, 50, 1], [-2.026075839996338, 1.047541220802291, 0.13328523384166169, 0, 0, 50, 18, 1], [-1.7705413273402624, 0.97206134208362982, 0.099279340335519994, 0, 0, 18, 65, 1], [-3.4620275497436523, 1.2762789110948516, 0.034910335335743725, 0, 0, 65, 21, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#after fixing word2vec\n",
    "b=[[-2.9132102966308593, 0.8442268296235873, 0.80551329359139057, 0, 0, 0, 25, 20, 1], [-2.3162093692355685, 0.86841709351845175, 0.86530378447712386, 0, 0, 0, 20, 50, 1], [-2.026075839996338, 0.80852565345349403, 0.70081317188750281, 0, 0, 0, 50, 18, 1], [-1.7705413273402624, 1.1240061836730386, 0.57432313199626739, 0, 0, 0, 18, 65, 1], [-3.4620275497436523, 0.87970654352248356, 0.93723039783182216, 0, 0, 0, 65, 21, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.9132102966308593, 0.8442268296235873, 0.8055132935913906, 0, 0, 0, 25, 20, 1]\n",
      "[-2.3162093692355685, 0.8684170935184518, 0.8653037844771239, 0, 0, 0, 20, 50, 1]\n",
      "[-2.026075839996338, 0.808525653453494, 0.7008131718875028, 0, 0, 0, 50, 18, 1]\n",
      "[-1.7705413273402624, 1.1240061836730386, 0.5743231319962674, 0, 0, 0, 18, 65, 1]\n",
      "[-3.4620275497436523, 0.8797065435224836, 0.9372303978318222, 0, 0, 0, 65, 21, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in b:print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    # average all of the word vectors in a sentence\n",
    "    words=words.split(' ')\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    for word in words:\n",
    "        nwords = nwords + 1.\n",
    "        featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "fname='rock_train.w2v'\n",
    "model = Word2Vec.load(fname)\n",
    "v1=make_feature_vec('I love you',model,100)\n",
    "v2=make_feature_vec('I love you baby',model,100)\n",
    "v3=make_feature_vec('Like Iâ€™m a broken relate',model,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/zangsir/repo/songLyrics'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0280046085098\n",
      "0.99573738118\n",
      "0.988993420945\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "print cosine(v1,v2)\n",
    "print cosine(v1,v3)\n",
    "print cosine(v2,v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01025271,  0.21873002, -0.00071767,  0.32168323, -0.42537275,\n",
       "       -0.45480549,  0.03541482, -0.25030354, -0.03485415, -0.14599648,\n",
       "       -0.13149096,  0.31597829, -0.08253186, -0.34015653,  0.00714215,\n",
       "        0.16426595,  0.28381777,  0.26444501,  0.02581098, -0.31495187,\n",
       "       -0.02961343, -0.29729041, -0.03097518,  0.07565158,  0.07872936,\n",
       "       -0.02143859,  0.16349572,  0.14878665, -0.40616712,  0.04453164,\n",
       "        0.41386622, -0.00784189, -0.31740722, -0.06529135, -0.44250783,\n",
       "        0.15238042, -0.0681546 , -0.15394318,  0.27342334, -0.25670004,\n",
       "       -0.08551664,  0.01297334, -0.68062192, -0.10403197, -0.39571747,\n",
       "       -0.08757023, -0.42382604, -0.24188383, -0.01875314,  0.16733015,\n",
       "        0.11461136, -0.08599471,  0.1772359 , -0.00927137, -0.00741462,\n",
       "       -0.05806764,  0.23455366, -0.06418875, -0.3586829 ,  0.01317938,\n",
       "        0.1891005 ,  0.28175753, -0.1886007 ,  0.26117644,  0.32812303,\n",
       "       -0.20934913,  0.02118781, -0.17306197,  0.41500264,  0.16750869,\n",
       "       -0.17757915, -0.08152993, -0.20289017, -0.15833935,  0.08544414,\n",
       "       -0.28757456,  0.08365934,  0.30717033,  0.21651155,  0.48627424,\n",
       "       -0.18977614,  0.15312445, -0.11570681, -0.21961409,  0.11814997,\n",
       "       -0.10364897, -0.25870773, -0.04999943, -0.21702918, -0.29854414,\n",
       "       -0.20516469,  0.33307999, -0.09736338,  0.39081708,  0.28234577,\n",
       "        0.11015064,  0.06521861, -0.27465209, -0.2243758 , -0.15189926], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"relate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
