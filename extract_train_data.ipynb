{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building train data\n",
    "\n",
    "-for a line, extract features based on this line and previous line, label=1\n",
    "\n",
    "-for a line, extract features based on this line and a random line, label=0\n",
    "\n",
    "-generate a large number of sents, for a low loglik generated line (bottom ranked K in the normalized-loglik list), \n",
    "\n",
    "-extract features for this line and a random line from the corpus, label=0\n",
    "\n",
    "-do we need to generate a lot of sentences for the negative examples?\n",
    "\n",
    "-we could also impose some hard coded constraints on the loglik threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11893\n"
     ]
    }
   ],
   "source": [
    "f=open('rank/test/lyrics_test_data_clean.txt','r').read().split('\\n\\n')\n",
    "print len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "But don't leave your keys for me, open the door\n",
      "Nobody's getting hurt here if nobody cares\n",
      "Nobody's getting lost if nobody's going anywhere\n",
      "---------\n",
      "And darling I know that you can't be sure of anything anymore\n",
      "---------\n",
      "I wanna love you like it ain't no secret\n",
      "Like I'm not ashamed to show\n",
      "Nor would I ever, oh never never\n",
      "Oh never let you go\n",
      "I'll never let you go\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "for i in f[:3]:\n",
    "    print i\n",
    "    print '---------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import gensim,os\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.models import Doc2Vec\n",
    "import pronouncing\n",
    "\n",
    "\n",
    "\n",
    "def is_rhyme(word1,word2):\n",
    "    word1=re.sub(u'[^A-Za-z]','',word1)\n",
    "    word2=re.sub(u'[^A-Za-z]','',word2)\n",
    "    return int(word1 in pronouncing.rhymes(word2))\n",
    "    \n",
    "def get_loglik_norm(sent,LM):\n",
    "    model = kenlm.LanguageModel(LM)\n",
    "    loglik_norm=model.score(sent)/len(sent.split(' '))\n",
    "    return loglik_norm\n",
    "\n",
    "def get_d2v_dist(sent1,sent2,model):\n",
    "    model = Doc2Vec.load(model)\n",
    "    a=model.infer_vector(sent1.split(' '))\n",
    "    b=model.infer_vector(sent2.split(' '))\n",
    "    return cosine(a,b)\n",
    "\n",
    "\n",
    "def make_feature_vec(words, model, num_features):\n",
    "    # average all of the word vectors in a sentence\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def get_w2v_dist(sent1,sent2,model):\n",
    "    v1=make_feature_vec(sent1,model,100)\n",
    "    v2=make_feature_vec(sent2,model,100)\n",
    "    return cosine(v1,v2)\n",
    "\n",
    "\n",
    "def extract_features_positive(passage,LM,w2v_model,d2v_model):\n",
    "    \"\"\"extract feature from one passage\"\"\"\n",
    "    # a passage is a consecutive set of lines without a blank line in between. we extract features with these pairs \n",
    "    # of lines as prev and next lines. they're a more coherent unit. The passages is obtained by methods above, \n",
    "    # namely, splitting the training file by '\\n\\n'\n",
    "    line_list=passage.split('\\n')\n",
    "    line_list=[i for i in line_list if i!='']\n",
    "    features=['loglik_norm','d2v_dist','w2v_dist','rhyme_prev','rhyme_current','num_syl_cur','num_syl_prev','class']\n",
    "    pos_feature_vec=[]\n",
    "    for i in range(1,len(line_list)):\n",
    "        #extract features from the current and prev line\n",
    "        prev=line_list[i-1]\n",
    "        current=line_list[i]\n",
    "        #features\n",
    "        loglik_norm=get_loglik_norm(current,LM)#LM='train3.lm'\n",
    "        d2v_dist=get_d2v_dist(prev,current,d2v_model)\n",
    "        w2v_dist=get_w2v_dist(prev,current,w2v_model)\n",
    "        rhyme_prev=is_rhyme(prev.split(' ')[-1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"But don't leave your keys for me, open the door\",\n",
       " \"Nobody's getting hurt here if nobody cares\",\n",
       " \"Nobody's getting lost if nobody's going anywhere\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2v_model='rock_train.d2v'\n",
    "w2v_model='rock_train.w2v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
